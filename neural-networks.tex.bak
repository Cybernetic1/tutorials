\documentclass[orivec]{llncs}
\usepackage{graphicx}
\usepackage{amsmath}			% for "cases"
\usepackage{amsfonts}		% for frakur fonts
\usepackage{mathrsfs}		% for curly "E" error symbol
\usepackage{float}
\usepackage[most]{tcolorbox}	% for wrapping example in color box
% \usepackage{wrapfig}			% wrap figure beside text, used in example
\usepackage{tikz-cd}			% commutative diagrams
\usepackage{tikz}
\usepackage{amssymb}			% for \multimap \updownarrow \bigstar \varnothing
\usepackage{sectsty}			% change section color
% \usepackage{turnstile}		% longer turnstiles
\usepackage{wasysym}			% smileys
\usepackage[normalem]{ulem}	% underline with line breaks: /uline
\usepackage{hyperref}		% refs, links become clickable
\usepackage[]{algorithm2e}	% algorithms

\usepackage{geometry}		% change paper size
\geometry{
  a4paper,         % or letterpaper
  textwidth=18cm,  % llncs has 12.2cm
  textheight=27cm, % llncs has 19.3cm
  heightrounded,   % integer number of lines
  hratio=1:1,      % horizontally centered
  vratio=2:3,      % not vertically centered
}
\usepackage[fontsize=13pt]{scrextend}

% *************** Delete when not using Chinese or colors **********************
\usepackage{xeCJK}
\setCJKmainfont[BoldFont=SimHei,ItalicFont=KaiTi]{SimSun}
\usepackage{color}
\definecolor{cerulean}{RGB}{100,100,200}
\definecolor{darkgreen}{RGB}{10,130,10}
%\newcommand{\emp}[1]{\textbf{\textcolor{Cerulean}{#1}}}
\newcommand{\emp}[1]{\textbf{#1}}
\definecolor{grey}{rgb}{0.9,0.9,0.9}  % grey

% \chapterfont{\color{blue}}		% sets colour of chapters
\sectionfont{\color{blue}} 
\subsectionfont{\color{blue}} 
\subsubsectionfont{\color{blue}} 
\setcounter{secnumdepth}{3}		% use numbers in subsubsections
% \renewcommand\thesection{}		% hide section numbers (has bad indent effect)

\let\emptyset\varnothing			% more beautiful empty set symbol
\newcommand{\vect}[1]{\boldsymbol{#1}}
\newcommand*\sigmoid{\vcenter{\hbox{\includegraphics{sigmoid.png}}}}
\newcommand*\KB{\vcenter{\hbox{\includegraphics{KB-symbol.png}}}}
\newcommand*\NN{\vcenter{\hbox{\includegraphics{NN-symbol.png}}}}
\newcommand*\invsigmoid{\vcenter{\hbox{\includegraphics{inverse-sigmoid.png}}}}
\newcommand{\invW}{\, \rotatebox[origin=c]{90}{W}}
\newcommand{\invw}{\, \rotatebox[origin=c]{90}{w}}
\newcommand*\rectifier{\vcenter{\hbox{\includegraphics{rectifier.png}}}}
\newcommand{\dashh}{\textemdash~}
\newcommand{\tab}{\hspace*{1cm}}

\newcommand{\tikzmark}[1]{\tikz[overlay,remember picture] \node (#1) {};}

\let\labelitemi\labelitemii

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\interfootnotelinepenalty=10000

% ***** Boxed variables inside math equations
% \newcommand*{\boxedcolor}{black}
\makeatletter
% \renewcommand{\boxed}[1]{\textcolor{\boxedcolor}{%
% \fbox{\normalcolor\m@th$\displaystyle#1$}}}
% \setlength{\fboxsep}{1pt}
\renewcommand{\boxed}[1]{\fbox{\m@th$\displaystyle\scalebox{0.9}{#1}$} \,}
\makeatother

\overfullrule=0mm

\newsavebox{\MyName}
\savebox{\MyName}{\includegraphics[scale=0.6]{YKY.png}}

\title{什么是神经网络？\\
{\normalsize（中学数学程度可懂）}}
\titlerunning{What are neural networks?}
\author{\usebox{\MyName} (King-Yin Yan)
% \\ \footnotesize{General.Intelligence@Gmail.com}
}
\institute{General.Intelligence@Gmail.com}

\begin{document}

\maketitle
\setlength{\parindent}{0em}
% \setlength{\parskip}{2.8ex plus0.8ex minus0.8ex}
\setlength{\parskip}{2.8ex}

%\begin{abstract}
%\end{abstract}

人工智能中最重要的 3 大技术是：
\begin{itemize}
 \item 逻辑
 \item 神经网络
 \item 进化
\end{itemize}
神经网络属於统计学习 (statistical learning) 的一种，这些方法将 vector space 中的某些「点」分类。

Deep learning 的意思，简单来说，是「\textbf{很多层}的神经网络」。 深度学习是目前人工智能中最备受瞩目的一种技巧。

\section*{生物的神经细胞}

温习一下在中学时学过的生物学 {\Large \smiley}

这是一粒真的神经元：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.4]{neuron.png}}}
\end{equation}
细胞周围的 dendrites \textbf{收集}电信号，当这些脉冲信号的\textbf{总和}超过某一阀值 (threshold) 时，会\textbf{发射} (fire) 一个电脉冲信号，从 axon 输出到另一神经元：

\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.5]{neuron-firing.png}}}
\end{equation}
数学上我们将这过程极度简化，变成这样的模型 (model)：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.75]{neuron-model.png}}}
\end{equation}
意思是： 将每个输入加权 (weighted) 加起来，然后经过一个 $\sigmoid$ 形状的函数输出。 

用数式表示：
\begin{equation}
\boxed{output} \; y = \sigmoid \; [ \; \sum_i (w_i \; x_i) \; ]
\end{equation}
其中 $\sigmoid$ = sigmoid 函数是：
\begin{equation}
\sigmoid (x) = \frac{1}{1 + e^{-x}}
\end{equation}
它的形状是这样的：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.55]{sigmoid-graph.png}}}
\end{equation}
它代表在左边没有信号 (0 = nothing)，在右边的信号值是 1 = ``yes''。

其实也可以用以下这些函数模拟 threshold 的作用：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.5]{../AGI-17/activation-functions.png}}}
\end{equation}

\begin{tcolorbox}[breakable, enhanced]
\textbf{Fun fact \#1}\\

神经细胞的表面布满了 sodium-potassium (Na$^+$/K$^+$) channels，它们用 ATP （adenosine triphosphate, 细胞的能量来源）以 3 Na$^+$ : 2 K$^+$ 的比例将 ions「泵」到细胞内，造成电压差。 这些蓄势待发的电压，当电压超过 threshold 时某些 channels 打开，造成 ``action potential''。 这个现象可以用微分方程描述，即著名的 Hodgkin-Huxley 方程，及其简化版本 FitzHugh-Nagumo 方程。\\

这 action potential 最特别的地方是： 它是一个 \textbf{all-or-nothing} effect，亦即是说，如果输入总和低於阀值，则输出信号是平的 (zero)。 为什么要这样呢？  因为人的脑袋是由一些水母那样的\textbf{多细胞}生物进化出来的，这些细胞之间慢慢学会用电信号 communicate，但它们在一滩水那样的环境下通讯，\textbf{噪音}很大。 直到现在人脑仍然是像一碗汤水那样的环境，而且人要活动，脑部不停有微小震动，造成\textbf{heat noise}。 为了要在噪音中运作，必须有机制将噪音\textbf{抑制}下去，这就是 all-or-nothing 的原因。 换句话说，人脑的意识，其资讯是\textbf{有限}的，就像电脑一样，并没有什么神秘。
\end{tcolorbox}

\begin{tcolorbox}[breakable, enhanced]
\textbf{Fun fact \#2} \\

神经细胞的\textbf{细胞膜}是由 lipid bi-layer 构成，它的成份是脂肪和胆固醇 (cholesterol)。 胆固醇的作用是稳固细胞膜结构，每粒细胞都需要胆固醇。 脑里面的神经线全都是用细胞膜组成，所以脑基本上都是脂肪和胆固醇。  尤其是中国人吃的猪脑，它的胆固醇含量是所有食物中最高的，高出鸡蛋很多倍！ \\

那层 myelin sheath 是脊椎类动物才有的，它好像电线外面的胶套，作用是加快神经传播的速度。 八爪鱼是无脊椎动物，所以它的头很大，牠比其他同类聪明，但脊椎类动物的脑袋可以较细小也达到同样的智力水平。
\end{tcolorbox}

\begin{tcolorbox}[breakable, enhanced]
\textbf{Fun fact \#3} \\

神经信号传递到末梢 (synapse)，不再用电传递，而是用化学分子 neuro-transmitter。 这些分子种类很多，例如抗抑郁药物常常提到的 serotonin 和 dopamine。 但其实最常见的 neurotransmitter 是 glutamate，它是所有动物的神经系统的主要通讯分子。 植物没有神经系统，所以植物里面没有 glutamate。 人类喜欢吃肉，所以进化出对肉类的味觉，特别喜欢 glutamate 的味道。 有个日本科学家发现了海藻内有一种物质，加进食物中可以做到肉的鲜味效果。  其实这种物质就是 glutamate，也就是「味精」。 所以味精其实对人体没有害，只是经常吃味精而不吃真的肉，会导致营养不均衡。 
\end{tcolorbox}

\section*{一粒神经元的几何解释}

以前说过，\textbf{机器学习}的目标通常是将空间上的某些「点」\textbf{分类}：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{ML-example.png}}}
\end{equation}

例如，在\textbf{机器视觉}中，一张图像可以有几万个 pixels，每个 pixel 是一个\textbf{维度}，它的\textbf{颜色}就是这个维度上的\textbf{座标值}。 整个空间就是\textbf{所有图像}的空间，\textbf{每一点}是一个图像，这种空间的维数很高（维数就是一张图像的 pixels 个数）。 通常我们讲解时用 2 维或 3 维空间，读者们要运用想像力幻想一下很高维的情况。

在中学数学中有学过，一条\textbf{直线}的方程是这形式的：
\begin{eqnarray}
& \mbox{\footnotesize \color{red}{constants}} \tikzmark{constants} \nonumber \\
\nonumber \\
& {\color{red}{a}} \tikzmark{constA} x \tikzmark{varX} + {\color{red}{b}} \tikzmark{constB} y \tikzmark{varY} + {\color{red}{c}} \tikzmark{constC} = 0 \\
\nonumber \\
& \mbox{\footnotesize variables} \tikzmark{variables} \nonumber
\begin{tikzpicture}[overlay,remember picture]
  \draw[red] (constants.center) +(-30pt,-4pt) -- ([shift={(-3pt,8pt)}]constA.center);
  \draw[red] (constants.center) +(-20pt,-4pt) -- ([shift={(-1pt,10pt)}]constB.center);
  \draw[red] (constants.center) +(-10pt,-4pt) -- ([shift={(-1pt,8pt)}]constC.center);
  \draw (variables.center) +(-29pt,8pt) -- ([shift={(-4pt,-2pt)}]varX.center);
  \draw (variables.center) +(-15pt,8pt) -- ([shift={(-4pt,-4pt)}]varY.center);
\end{tikzpicture}
\end{eqnarray}

它的\textbf{几何}解释是这样的：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.75]{linear-inequality.png}}}
\end{equation}
在直线上，那直线的方程 $= 0$。 直线将它身在的空间\textbf{分割}成两半： 一边 $> 0$，另一边 $< 0$。

推广到 3 维空间的情况，我们有一个\textbf{平面}的方程： 
\begin{equation}
{\color{red}{a}}x + {\color{red}{b}}y + {\color{red}{c}}z + {\color{red}{d}} = 0
\end{equation}
它同样将空间\textbf{分割}成两半，一半 $> 0$，另一半 $< 0$：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.75]{linear-inequality-2.png}}}
\end{equation}

在任意 $n$-维空间，每一点记作 $\vect{x} = (x_1, x_2, ..., x_n)$，一个\textbf{超平面} (hyper-plane) 将该空间分割成两半，它的方程是： 
\begin{equation}
{\color{red}{a_1}}x_1 + {\color{red}{a_2}}x_2 + .... + {\color{red}{a_n}}x_n + {\color{red}{a_0}} = 0
\label{eqn:general-linear}
\end{equation}

\begin{tcolorbox}[breakable, enhanced]
注意： 一块超平面的维数是多少？ 在平面空间里它是一条线（1 度空间），在立体空间里它是平面（2 度空间）， 一般来说，在 $n$-维空间里的超平面是一个 $n - 1$ 维的物体，$(n-1)$ 又叫作 co-dimension 1，意思是说 ambient space 的维数是 $n$，方程 (\ref{eqn:general-linear}) 减少了一个\textbf{自由度} (degree of freedom)，所以\textbf{服从}这等式的物体内，只有 $n - 1$ 个自由度。 
\end{tcolorbox}

现在可以看到超平面和神经元之间有些相似，因为神经元在未经过 $\sigmoid$ 之前，就是一个 \textbf{线性组合}： 
\begin{eqnarray}
& \mbox{\footnotesize \color{red}{linear combination}} \nonumber \\
\boxed{output} \; y = \sigmoid & [ \; \overbrace{\sum_i (w_i \; x_i)} \; ]
\end{eqnarray}
换句话说： \uline{每粒神经元构成一超平面，它将空间切割成两半}。

加了 $\sigmoid$ 之后怎样？ 未有 $\sigmoid$ 时，分割的两边分别是 $> 0$ 和 $< 0$，如果将颜色看作是「强度」，强度是逐渐变化的：  （右边表示从侧面看，立体）
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{linear-inequality-3.png}}}
\end{equation}
加了 sigmoid 之后，用 1 代表「yes」，0 代表「no」，则两边的对比加强了，亦即更\textbf{两极化}、「非此即彼」：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{linear-inequality-4.png}}}
\end{equation}

\section*{> 1 粒神经元的几何解释}

如果有 $> 1$ 粒神经元（在同一层上），例如 3 粒：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{linear-inequalities-1.png}}}
\label{simple-network-layer}
\end{equation}
注意： 座标是 $(x_1, x_2) = $ 输入，输出是 $(y_1, y_2, y_3)$，分别用 3 条{\color{red}{虚线}}代表。 这\textbf{一层}神经网络的 network topology 如 (\ref{simple-network-layer}) 右图所示（每粒神经元没有画出 $\sum$ 和 $\sigmoid$）。

每粒神经元可以选择某一边为「yes」，这些选择的 conjunction 可以形成不同的形状，例如以下两种： 
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{linear-inequalities-2.png}}}
\end{equation}
亦注意在右图中，可以出现几个 disjoint regions（在空间上分离的）。

很明显，可以用神经元将输入空间的点进行切割和分类，达到\textbf{机器学习}的目的。

\section*{一层神经网络的几何解释}

%简单来说，当神经网络的\textbf{层数}增加，在输入空间的切割形状会变得更为\textbf{复杂}，所以多层的神经网络有能力学习出非常复杂的 patterns，这就是\textbf{深度学习}的优势。

一层神经网络的数学形式是：
\begin{equation}
\vect{y} = \sigmoid [ W \vect{x} ] 
\end{equation}
其中 W 是矩阵，亦即 \textbf{线性变换}； $\sigmoid$ 是 \textbf{非线性变换}。

首先要明白什么是 \textbf{linear transformation}：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{linear-transformation.png}}}
\end{equation}
线性变换可以将正方形「扯」成平行四边形，也包括位置上的\textbf{旋转} (rotation) 和\textbf{平移} (translation)。 但直线仍变为直线，故名。

另外要明白 $\sigmoid$ 变换的形状： （以下是 $x$ 和 $y$ 座标都被 sigmoid 变换）
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.75]{sigmoid-transforms.png}}}
\end{equation}
可以这样理解 $\sigmoid$： 它将平面上的点「扯向」4个顶点，顶点是\textbf{吸引子} (attractors)，所以我变了「国字口面」。 换句话说，hyper-cube 内部和外部的点，均被吸引到它的顶点上，这些顶点代表各种 yes / no 组合。

\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{NN-1-layer.png}}}
\end{equation}

为简化讨论，现在只考虑 2 粒神经元的\textbf{输入}和\textbf{输出}空间（都是 2 度空间的平面）：
\begin{equation}
\vcenter{\hbox{\includegraphics[scale=0.6]{linear-inequalities-3.png}}}
\end{equation}
因为 $\sigmoid$ 的缘故，输出的位置会趋近 hyper-cube 的那些 \textbf{顶点} ({\color{red}{\textbullet}})。 这些顶点对应於输入空间中被割开的 regions。 例如 {\color{cerulean}{蓝色}}那块 region 对应於：
\begin{equation}
(y_1 = \mbox{yes}, y_2 = \mbox{yes}) \quad \Rightarrow \quad (1,1)
\end{equation}
当输入位置随{\color{darkgreen}{绿色}}线游荡时，输出会在 hyper-cube 的顶点之间跳来跳去。 妳可能觉得这样移动很无聊（因为顶点个数不多），但当神经元的个数 $n$ 增加时，hyper-cube 的顶点个数会以 $2^n$ 的速度增长。 

\section*{多层神经网络的几何解释}

% \href{http://genifer.googlecode.com/files/Genifer - induction (30 July 2012).pdf}{inductive learning (PDF, in English)}

\centering{ --- 完 --- }

%\bibliographystyle{plain} % or number or aaai ...
%\bibliography{AGI-book}

\end{document}
